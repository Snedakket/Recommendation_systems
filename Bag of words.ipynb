{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92597021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "# Система позволяет искать тексты из ограниченного количества документов, тесно связаны с заданной темой. \n",
    "# Чтобы использовать возможности NLP, объединим методологию поиска с методами оценки семантического сходства."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af92285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>movie</th>\n",
       "      <th>rating_ball</th>\n",
       "      <th>overview</th>\n",
       "      <th>director</th>\n",
       "      <th>screenwriter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Побег из Шоушенка</td>\n",
       "      <td>9.111</td>\n",
       "      <td>Бухгалтер Энди Дюфрейн обвинён в убийстве собс...</td>\n",
       "      <td>Фрэнк Дарабонт</td>\n",
       "      <td>Фрэнк Дарабонт;  Стивен Кинг</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Зеленая миля</td>\n",
       "      <td>9.062</td>\n",
       "      <td>Пол Эджкомб — начальник блока смертников в тюр...</td>\n",
       "      <td>Фрэнк Дарабонт</td>\n",
       "      <td>Фрэнк Дарабонт;  Стивен Кинг</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Форрест Гамп</td>\n",
       "      <td>8.913</td>\n",
       "      <td>От лица главного героя Форреста Гампа; слабоум...</td>\n",
       "      <td>Роберт Земекис</td>\n",
       "      <td>Эрик Рот;  Уинстон Грум</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Список Шиндлера</td>\n",
       "      <td>8.817</td>\n",
       "      <td>Фильм рассказывает реальную историю загадочног...</td>\n",
       "      <td>Стивен Спилберг</td>\n",
       "      <td>Стивен Зеллиан;  Томас Кенилли</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1+1</td>\n",
       "      <td>8.807</td>\n",
       "      <td>Пострадав в результате несчастного случая; бог...</td>\n",
       "      <td>Оливье Накаш;  Эрик Толедано</td>\n",
       "      <td>Оливье Накаш;  Филипп Поццо ди Борго;  Эрик Т...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating               movie  rating_ball  \\\n",
       "0       0  Побег из Шоушенка         9.111   \n",
       "1       1       Зеленая миля         9.062   \n",
       "2       2       Форрест Гамп         8.913   \n",
       "3       3    Список Шиндлера         8.817   \n",
       "4       4                1+1         8.807   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Бухгалтер Энди Дюфрейн обвинён в убийстве собс...   \n",
       "1  Пол Эджкомб — начальник блока смертников в тюр...   \n",
       "2  От лица главного героя Форреста Гампа; слабоум...   \n",
       "3  Фильм рассказывает реальную историю загадочног...   \n",
       "4  Пострадав в результате несчастного случая; бог...   \n",
       "\n",
       "                        director  \\\n",
       "0                 Фрэнк Дарабонт   \n",
       "1                 Фрэнк Дарабонт   \n",
       "2                 Роберт Земекис   \n",
       "3                Стивен Спилберг   \n",
       "4   Оливье Накаш;  Эрик Толедано   \n",
       "\n",
       "                                        screenwriter  \n",
       "0                       Фрэнк Дарабонт;  Стивен Кинг  \n",
       "1                       Фрэнк Дарабонт;  Стивен Кинг  \n",
       "2                            Эрик Рот;  Уинстон Грум  \n",
       "3                     Стивен Зеллиан;  Томас Кенилли  \n",
       "4   Оливье Накаш;  Филипп Поццо ди Борго;  Эрик Т...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "data = 'D:/Python_Projects/Recomendation_Systems/kinopoisk-top250.csv'\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "df = df[~df.isna()]\n",
    "\n",
    "df = df.iloc[:5]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b37543cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning overview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sneda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sneda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sneda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Удаляем слишком короткие и слишком длинные предложения из набора данных с помощью MIN_WORDS и MAX_WORDS. \n",
    "#Удаляем стоп слова, которые не помогают извлечь специфику предложения.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "MIN_WORDS = 5\n",
    "MAX_WORDS = 100\n",
    "\n",
    " #Очистка.Перевод строк в нижний регистр, удаление символов не являющихся словами (знаки препинания, кавычки и т.д.)\n",
    "    \n",
    "PATTERN_S = re.compile(\"\\'s\")  # matches `'s` from text  \n",
    "PATTERN_RN = re.compile(\"\\\\r\\\\n\") #matches `\\r` and `\\n`\n",
    "PATTERN_PUNC = re.compile(r\"[^\\w\\s]\") # matches all non 0-9 A-z whitespace \n",
    "\n",
    "def clean_text(text):\n",
    "  \n",
    "    text = str(text).lower()  # lowercase text\n",
    "    # replace the matched string with ' '\n",
    "    text = re.sub(PATTERN_S, ' ', text)\n",
    "    text = re.sub(PATTERN_RN, ' ', text)\n",
    "    text = re.sub(PATTERN_PUNC, ' ', text)\n",
    "    return text\n",
    "\n",
    "#Лемматизация, токенизация, обрезка и удаление стоп слов.\n",
    " \n",
    "def tokenizer(overview, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True):\n",
    "  \n",
    "    if lemmatize:\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        tokens = [stemmer.lemmatize(w) for w in word_tokenize(overview)]\n",
    "    else:\n",
    "        tokens = [w for w in word_tokenize(overview)]\n",
    "    token = [w for w in tokens if (len(w) > min_words and len(w) < max_words\n",
    "                                                        and w not in stopwords)]\n",
    "    return tokens\n",
    "\n",
    "#Уадление неверных символов (в новой колонке clean_sentence)\n",
    "#Лемматизация, токенизация слов в список слов (в новой колонке tok_lem_sentence )\n",
    "\n",
    "def clean_overview(df):   \n",
    "    \n",
    "    print('Cleaning overview')\n",
    "    df['clean_overview'] = df['overview'].apply(clean_text)\n",
    "    df['tok_lem_overview'] = df['clean_overview'].apply(\n",
    "        lambda x: tokenizer(x, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS))\n",
    "    return df\n",
    "    \n",
    "df = clean_overview(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c1db409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция, которая будет ранжировать лучшие рекомендации с учетом косинусного расстояния между векторами\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Использует сумму косинусного расстояния для всех токенов и возвращает наилучшее совпадение\n",
    "def extract_best_indices(m, topk, mask=None):\n",
    " \n",
    "    # Возвращает сумму по всем токенам косинуса для каждого предложения\n",
    "    if len(m.shape) > 1:\n",
    "        cos_sim = np.mean(m, axis=0) \n",
    "    else: \n",
    "        cos_sim = m\n",
    "    index = np.argsort(cos_sim)[::-1] # от самого высокого idx до наименьшего балла\n",
    "    if mask is not None:\n",
    "        assert mask.shape == m.shape\n",
    "        mask = mask[index]\n",
    "    else:\n",
    "        mask = np.ones(len(cos_sim))\n",
    "    mask = np.logical_or(cos_sim[index] != 0, mask) #исключает косинусное расстояние 0\n",
    "    best_index = index[mask][:topk]  \n",
    "    return best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca40967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneda\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\sneda\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'ha', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>rating_ball</th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Форрест Гамп</td>\n",
       "      <td>8.913</td>\n",
       "      <td>От лица главного героя Форреста Гампа; слабоум...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1+1</td>\n",
       "      <td>8.807</td>\n",
       "      <td>Пострадав в результате несчастного случая; бог...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Список Шиндлера</td>\n",
       "      <td>8.817</td>\n",
       "      <td>Фильм рассказывает реальную историю загадочног...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              movie  rating_ball  \\\n",
       "2     Форрест Гамп         8.913   \n",
       "4              1+1         8.807   \n",
       "3  Список Шиндлера         8.817   \n",
       "\n",
       "                                            overview  \n",
       "2  От лица главного героя Форреста Гампа; слабоум...  \n",
       "4  Пострадав в результате несчастного случая; бог...  \n",
       "3  Фильм рассказывает реальную историю загадочног...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "# После расчета TF-IDF (веса слов с учетом их частоты в тексте, нормированной на частоту во всем корпусе документов), \n",
    "# сгенерируем вектор встраивания для каждого описания фильма. Эти функции хранятся в матрице функций tfidf_mat, \n",
    "# где каждая строка это запись описания фильма, встроенная в вектор функций. \n",
    "# Когда получим запрос из пользовательского ввода, встроим его в одно и то же векторное пространство и сравним\n",
    "# один за другим функцию предложения запроса embed_query с векторами предложений матрицы внедрения tfidf_mat.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_recommendations_tfidf(overview, tfidf_mat):\n",
    "    \n",
    "      # Вставить запрос\n",
    "    tokens_query = [str(tok) for tok in tokenizer(overview)]\n",
    "    embed_query = vectorizer.transform(tokens_query)\n",
    "    \n",
    "    # Создать список схожести между запросом и датасетом\n",
    "    mat = cosine_similarity(embed_query, tfidf_mat)\n",
    "    \n",
    "    # Лучшее косинусное расстояния для каждого токена независимо\n",
    "    best_index = extract_best_indices(mat, topk=3)\n",
    "    return best_index\n",
    "\n",
    "# адаптация стоп слов\n",
    "token_stop = tokenizer(' '.join(stopwords.words('english')), lemmatize=False)\n",
    "\n",
    "# Fit TFIDF\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer) \n",
    "tfidf_mat = vectorizer.fit_transform(df['overview'].values)\n",
    "\n",
    "# Возврат лучших трех совпадений между запросом и датасетом\n",
    "test_overview = 'От лица главного героя Форреста Гампа' \n",
    "best_index = get_recommendations_tfidf(test_overview, tfidf_mat)\n",
    "\n",
    "display(df[['movie', 'rating_ball', 'overview']].iloc[best_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonGPU]",
   "language": "python",
   "name": "conda-env-PythonGPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
